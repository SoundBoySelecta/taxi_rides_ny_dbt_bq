DBT - Data Built Tool
Dbt is a transformation layer tool used mostly within Data ware houses (DWH) focusing on SQL based transformations.
Use cases: Data engineering practices have evolved, we focus on ELT versus, ETL. Raw data is extracted from 
sources and loaded into a data lake then transformed in a DWH. DBT is analytics focused, the closer the data gets to 
the end user analysts, there is value in smaller and higher quality data. Smaller in the sense for speed of efficient querying 
returning results faster and higher quality in that its clean data. Finally we create query optimized schemas prior to the data being 
ingested into analytics workflows, to minimize compute resources by not having to query large joined tables with may contain dirty data. 
The DBT workflow corresponds to (bronze, silver, gold)  medallion architecture, by segregating each layer into different models and applying SQL logic
to clean, tranform and re-model this data into star/snowflake type schemas, where we move away from OLTP but rather enforce OLAP design principales.
ELT also takes advantage of seperating compute and storage, where we can use compute resources to extract data from disparate systems 
and use a seperate storage scheme in this case a data lake.

Why do we need to separate compute and storage?
Scalability: We can scale each at differently, without impacting each other.
Flexibity: Different configurations and techs for each. Non dependant.
Cost-Effectiveness: We dont need a one size fit all scenario, so we can customize each so we dont over provision and under
utilize.
Performance Isolation: Independant components  = independant components = independant performance. 
Containerized orchestrators like K8 can spawn new compute instances if one crashes. Clou data storage and lakes
are fault tolerant by nature. 
Data durability and Avialibility: Lower risk of data loss and downtime, if compute component goes down, the storage
layer is still available and vise versa. Cloud systems replicate and distribute data across different region.
Data Portability: We can manage and migrate data between systems as needed.
Specialized workloads: catered solutions each component, can use object storage for unstructured and relation databases 
for structured, and use general purpose compute for processing.

Once our data is extracted from source and loaded into a DWH, we can start our DBT workflow.
Setting up a project on DBT is very straight forward: You have an option to run local or on cloud IDE. Once a cloud account is setup, do the following:
1. Create a project. Name project.
2. Add your DWH credentials (json service account), important here that you add the location field (which is <region>-<zone>), as the credential doesnt include it.
3. Link your GH account fully where you will be able to see all your repos or just clone a repo.
4. If you clone a repo, you inherit into DBT the same directory structure which you will see in file browser in DBT cloud.
5. Edit dbt_project.yml to modify project name and add the 'staging' model under models.
6. Create a sub-directory named 'staging' under models directory, this is where all your sql code will be.
7. Create a schema.yml file which will define to DBT your sources.
    - Name the source 'staging' and define the sources, project_id.dataset.table in Big Query is equivalent to databae.schema.table in DBT 
8. Once the tables are detected within BQ per your translation project_id.dataset.table =  database.schema.table, DBT creates a link to auto generate the associated model and save. 
This model is the basic building block of DBT, a simple sql file. So models are really sql scripts that are applied on remote data resources which shape and manipulate data, then save
that data as a view or table, but that data never comes into DBT, DBT just manages these models over exisitng and we sync this sql  code base with GH. So there are only text files.  
    - Note in demo the autogenerated models created an extra staging folder under the existing staging forlder, I think this is a flaw.
9. Once those first models are generated you can run dbt build
    - What does dbt build do? Long answer short it builds out the medalliaon layers as per your model speciifcations as we go from bronze - silver - gold. 
    A) We defined a source project_id.dataset.table, DBT provide a link in the schema.yml to auto generate a model (sql file).
    B) This auto generated model created a data set in BQ called dbt_beyond_analytics, then took the source tables and created a view (stg_green_tripdata and stg_yellow_tripdata).
    C) Dbt build also built al the modelsand tests inside project folders including model/example (I deleted it), without explicit instructions to do so.
     Versus just the build icon under the sql file.
    D) Jinja templating: 
        a) create a file in the macros directory (get_payment_type_description.sql), copy code from gh repo macros
        b) {# This is a comment #}
        c){%macro function_name(parameters)-%} macro is like def in python
        d) What did we do we? We input column payment_type into macro fuction as a parameter 
           and case tested it for casted int values thenreturned an string representation output.
            when 1 then 'Credit card'
            when 2 then 'Cash'
            when 3 then 'No charge'
            when 4 then 'Dispute'
            when 5 then 'Unknown'
            when 6 then 'Voided trip'
            else 'EMPTY'





